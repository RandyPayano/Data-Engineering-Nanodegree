{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import psycopg2\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Cluster Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_CLUSTER_TYPE</td>\n",
       "      <td>multi-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_NUM_NODES</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_CLUSTER_IDENTIFIER</td>\n",
       "      <td>dwhCluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_DB</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_DB_USER</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DWH_DB_PASSWORD</td>\n",
       "      <td>Passw0rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DWH_IAM_ROLE_NAME</td>\n",
       "      <td>dwhRole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Param       Value\n",
       "0        DWH_CLUSTER_TYPE  multi-node\n",
       "1           DWH_NUM_NODES           4\n",
       "2           DWH_NODE_TYPE   dc2.large\n",
       "3  DWH_CLUSTER_IDENTIFIER  dwhCluster\n",
       "4                  DWH_DB         dwh\n",
       "5             DWH_DB_USER     dwhuser\n",
       "6         DWH_DB_PASSWORD    Passw0rd\n",
       "7                DWH_PORT        5439\n",
       "8       DWH_IAM_ROLE_NAME     dwhRole"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('clusterparams.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":[\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":[DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Redshift, S3 and IAM clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\",\n",
    "                    region_name=\"us-west-2\",\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "redshift = boto3.client(\"redshift\",\n",
    "                    region_name=\"us-west-2\",\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource(\"s3\",\n",
    "                    region_name=\"us-west-2\",\n",
    "                    aws_access_key_id=config.get('AWS','KEY'),\n",
    "                    aws_secret_access_key=config.get('AWS','SECRET')\n",
    "                    )\n",
    "\n",
    "ec2 = boto3.resource(\"ec2\",\n",
    "                    region_name=\"us-west-2\",\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new IAM Role\n"
     ]
    }
   ],
   "source": [
    "# Create the IAM role\n",
    "try:\n",
    "    print('Creating a new IAM Role')\n",
    "    dwhRole = iam.create_role(\n",
    "           Path='/',\n",
    "            RoleName=\"thisisthename\",\n",
    "            Description='Allow Redshift clusters to call AWS services on your behalf.',\n",
    "            AssumeRolePolicyDocument=json.dumps(\n",
    "                {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "               'Version': '2012-10-17'})\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach Policy\n",
    "print('1.2 Attaching Policy')\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                      PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\",\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IAM role ARN\n",
    "print('1.3 Get the IAM role ARN')\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)[\"Role\"]['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING REDSHIFT CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        # hardware\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "        \n",
    "        # identifiers & credentials\n",
    "            DBName=DWH_DB,\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "            MasterUsername=DWH_DB_USER,\n",
    "            MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        # parameter for role (to allow s3 access)\n",
    "         IamRoles=[roleArn]\n",
    "       \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See cluster status\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_address = myClusterProps['Endpoint']['Address']\n",
    "print('Cluster Address:', cluster_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IamRoleArn = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print('IamRoleArn:', IamRoleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening an incoming TCP port to access the cluster ednpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName= defaultSg.group_name, \n",
    "        CidrIp='0.0.0.0/0',  \n",
    "        IpProtocol='TCP',  \n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DB Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configETL = configparser.ConfigParser()\n",
    "configETL.read_file(open('dwh.cfg'))\n",
    "LOG_DATA = configETL.get(\"S3\",\"LOG_DATA\")\n",
    "LOGPATH = configETL.get(\"S3\",\"LOG_JSONPATH\")\n",
    "SONG_DATA = configETL.get(\"S3\",\"SONG_DATA\")\n",
    "IAMROLE = configETL.get(\"IAM_ROLE\", \"ARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONNECTING TO CLUSTER DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*configETL['CLUSTER'].values()))\n",
    "cur = conn.cursor()\n",
    "cur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONNECTING to S3 \"udacitiy-dend\" Bucket and Preview list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data = [filename.key for filename in s3.Bucket(\"udacity-dend\").objects.filter(Prefix='song_data')]\n",
    "song_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = [filename.key for filename in s3.Bucket(\"udacity-dend\").objects.filter(Prefix='log_data')]\n",
    "log_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP TABLES IF EXISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_staging_events = \"DROP TABLE IF EXISTS staging_events\"\n",
    "drop_staging_songs = \"DROP TABLE IF EXISTS staging_songs\"\n",
    "drop_fact_songplay = \"DROP TABLE IF EXISTS fact_songplay\"\n",
    "drop_dim_users = \"DROP TABLE IF EXISTS dim_users\"\n",
    "drop_dim_songs = \"DROP TABLE IF EXISTS dim_songs\"\n",
    "drop_dim_artists = \"DROP TABLE IF EXISTS dim_artists\"\n",
    "drop_dim_time = \"DROP TABLE IF EXISTS dim_time\"\n",
    "\n",
    "tables_to_drop = [drop_staging_events,drop_staging_songs,drop_fact_songplay, \n",
    "                  drop_dim_users, drop_dim_songs,drop_dim_artists,drop_dim_time]\n",
    "\n",
    "for table in tables_to_drop:\n",
    "    cur.execute(table)\n",
    "    conn.commit()\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESIGNING STAGING, FACT & DIMENSION TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGING tables are used to stage before modeling into Star Schema\n",
    "\n",
    "create_staging_events = (\"\"\"CREATE TABLE IF NOT EXISTS staging_events(\n",
    "artist VARCHAR,\n",
    "auth VARCHAR,\n",
    "firstName VARCHAR,\n",
    "gender VARCHAR,\n",
    "itemInSession INTEGER,\n",
    "lastName VARCHAR,\n",
    "length FLOAT,\n",
    "level VARCHAR,\n",
    "location VARCHAR,\n",
    "method VARCHAR,\n",
    "page VARCHAR,\n",
    "registration BIGINT,\n",
    "sessionId INTEGER,\n",
    "song VARCHAR,\n",
    "status INTEGER,\n",
    "ts TIMESTAMP,\n",
    "userAgent VARCHAR,\n",
    "userId INTEGER\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "create_staging_songs = (\"\"\"CREATE TABLE IF NOT EXISTS staging_songs(\n",
    "num_songs VARCHAR,\n",
    "artist_id VARCHAR, \n",
    "artist_latitude FLOAT, \n",
    "artist_longitude FLOAT, \n",
    "artist_location VARCHAR, \n",
    "artist_name VARCHAR, \n",
    "song_id VARCHAR, \n",
    "title VARCHAR, \n",
    "duration FLOAT,\n",
    "year INT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "create_fact_songplay = (\"\"\"CREATE TABLE IF NOT EXISTS fact_songplay\n",
    "(\n",
    "songplay_id INTEGER IDENTITY(0,1) PRIMARY KEY sortkey,\n",
    "start_time TIMESTAMP,\n",
    "user_id INTEGER, \n",
    "level VARCHAR, \n",
    "song_id VARCHAR,\n",
    "artist_id VARCHAR,\n",
    "session_id INTEGER,\n",
    "location VARCHAR,\n",
    "user_agent VARCHAR)\n",
    "\"\"\")\n",
    "\n",
    "create_dim_users = (\"\"\"CREATE TABLE IF NOT EXISTS dim_users\n",
    "(\n",
    "user_id INTEGER PRIMARY KEY distkey,\n",
    "first_name VARCHAR,\n",
    "last_name VARCHAR,\n",
    "gender VARCHAR,\n",
    "level VARCHAR)\n",
    "\"\"\")\n",
    "\n",
    "create_dim_songs = (\"\"\"CREATE TABLE IF NOT EXISTS dim_songs\n",
    "(\n",
    "song_id VARCHAR PRIMARY KEY,\n",
    "title VARCHAR, \n",
    "artist_id VARCHAR distkey,\n",
    "year INTEGER, \n",
    "duration FLOAT)\n",
    "\"\"\")\n",
    "\n",
    "create_dim_artists = (\"\"\"CREATE TABLE IF NOT EXISTS dim_artists\n",
    "(\n",
    "artist_id VARCHAR PRIMARY KEY distkey,\n",
    "name VARCHAR, \n",
    "location VARCHAR, \n",
    "lattitude FLOAT, \n",
    "longitude FLOAT)\n",
    "\"\"\")\n",
    "\n",
    "create_dim_time = (\"\"\"CREATE TABLE IF NOT EXISTS dim_time\n",
    "(\n",
    "start_time TIMESTAMP PRIMARY KEY sortkey distkey, \n",
    "hour INTEGER, \n",
    "day INTEGER, \n",
    "week INTEGER, \n",
    "month INTEGER, \n",
    "year INTEGER, \n",
    "weekday INTEGER)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING STAGING, FACT & DIMENSION TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_to_create =[create_staging_events, create_staging_songs, create_fact_songplay, create_dim_users, create_dim_songs,\n",
    "                   create_dim_artists, create_dim_time]\n",
    "\n",
    "for table in tables_to_create:\n",
    "    cur.execute(table)\n",
    "    print(\"Table created\")\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COPY staging_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_staging_events = (\"\"\"\n",
    "COPY staging_events FROM {}\n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "COMPUPDATE OFF region 'us-west-2'\n",
    "TIMEFORMAT as 'epochmillisecs'\n",
    "TRUNCATECOLUMNS BLANKSASNULL EMPTYASNULL\n",
    "FORMAT AS JSON {};\n",
    "\"\"\").format(LOG_DATA, IAMROLE, LOGPATH)\n",
    "\n",
    "cur.execute(copy_staging_events)\n",
    "print(\"staging events copied\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIEW staging_events\n",
    "##  artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId\n",
    "\n",
    "query = cur.execute(\"\"\"SELECT * FROM staging_events\"\"\")\n",
    "for i in range(2):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COPY staging_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_staging_songs = (\"\"\"\n",
    "COPY staging_songs FROM {}\n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "COMPUPDATE OFF region 'us-west-2'\n",
    "FORMAT AS JSON 'auto'\n",
    "TRUNCATECOLUMNS BLANKSASNULL EMPTYASNULL\n",
    "\"\"\").format(SONG_DATA, IAMROLE)\n",
    "\n",
    "cur.execute(copy_staging_songs)\n",
    "print(\"staging songs copied\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIEW staging_songs\n",
    "## num_songs, artist_id,  artist_latitude ,  artist_longitude ,  artist_location,  artist_name,  song_id,  title,  duration, year\n",
    "\n",
    "query = cur.execute(\"\"\"SELECT * FROM staging_songs\"\"\")\n",
    "for i in range(2):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERT INTO FACT_songplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIEW QUERY B4 INSERTING\n",
    "query = cur.execute(\"\"\"SELECT DISTINCT e.ts,\n",
    "                e.userId as user_id,\n",
    "                e.level as level,\n",
    "                s.song_id as song_id,\n",
    "                s.artist_id as artist_id,\n",
    "                e.sessionId as session_id,\n",
    "                e.location as location,\n",
    "                e.userAgent as user_agent\n",
    "FROM staging_events e\n",
    "JOIN staging_songs s ON e.song = s.title AND e.artist = s.artist_name\n",
    "WHERE e.page='NextSong'\n",
    "\"\"\")\n",
    "\n",
    "for i in range(2):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"INSERT INTO fact_songplay (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "                                    SELECT DISTINCT e.ts,\n",
    "                                                    e.userId as user_id,\n",
    "                                                    e.level as level,\n",
    "                                                    s.song_id as song_id,\n",
    "                                                    s.artist_id as artist_id,\n",
    "                                                    e.sessionId as session_id,\n",
    "                                                    e.location as location,\n",
    "                                                    e.userAgent as user_agent\n",
    "                                    FROM staging_events e\n",
    "                                    JOIN staging_songs s ON e.song = s.title AND e.artist = s.artist_name\n",
    "                                    WHERE e.page='NextSong'\n",
    "                                  \"\"\")\n",
    "\n",
    "# Preview newly created fact_songplay table\n",
    "query = cur.execute(\"\"\"SELECT * FROM fact_songplay\"\"\")\n",
    "for i in range(3):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERT INTO dim_users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIEW QUERY B4 INSERTING\n",
    "query = cur.execute(\"\"\"SELECT DISTINCT userId as user_id,\n",
    "                firstName as first_name,\n",
    "                lastName as last_name,\n",
    "                gender as gender,\n",
    "                level as level\n",
    "FROM staging_events\n",
    "where userId IS NOT NULL;\n",
    "\"\"\")\n",
    "\n",
    "for i in range(3):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"INSERT INTO dim_users(user_id, first_name, last_name, gender, level)\n",
    "                        SELECT DISTINCT userId as user_id,\n",
    "                                        firstName as first_name,\n",
    "                                        lastName as last_name,\n",
    "                                        gender as gender,\n",
    "                                        level as level\n",
    "                        FROM staging_events\n",
    "                        where userId IS NOT NULL;\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Preview newly created dim_users table \n",
    "query = cur.execute(\"\"\"SELECT * FROM dim_users\"\"\")\n",
    "for i in range(3):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERT INTO dim_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIEW query b4 inserting\n",
    "query = cur.execute(\"\"\"SELECT DISTINCT song_id, title, artist_id, year, duration\n",
    "                    FROM staging_songs\n",
    "                    WHERE song_id IS NOT NULL\n",
    "                    \"\"\")\n",
    "for i in range(3):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"INSERT INTO dim_songs (song_id, title, artist_id, year, duration)\n",
    "                SELECT DISTINCT song_id, title, artist_id, year, duration\n",
    "                FROM staging_songs\n",
    "                WHERE song_id IS NOT NULL\n",
    "                \"\"\")\n",
    "\n",
    "\n",
    "# Preview newly created dim_users table \n",
    "query = cur.execute(\"\"\"SELECT * FROM dim_songs\"\"\")\n",
    "for i in range(3):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERTING INTO dim_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIEW query b4 inserting\n",
    "query = cur.execute(\"\"\"SELECT DISTINCT artist_id, e.artist as name, s.artist_location, s.artist_latitude, s.artist_longitude\n",
    "                        FROM staging_events e\n",
    "                        JOIN staging_songs s ON e.artist = s.artist_name\n",
    "                        WHERE e.artist IS NOT NULL\n",
    "                    \"\"\")\n",
    "for i in range(3):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"INSERT INTO dim_artists(artist_id, name, location, lattitude, longitude)\n",
    "                   SELECT DISTINCT artist_id, e.artist as name, s.artist_location, s.artist_latitude, s.artist_longitude\n",
    "                   FROM staging_events e\n",
    "                   JOIN staging_songs s ON e.artist = s.artist_name\n",
    "                   WHERE e.artist IS NOT NULL\n",
    "            \"\"\")\n",
    "\n",
    "# Preview newly created dim_artist table \n",
    "query = cur.execute(\"\"\"SELECT * FROM dim_artists\"\"\")\n",
    "for i in range(3):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERTING INTO dim_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview query to insert\n",
    "cur.execute(\"\"\"SELECT DISTINCT ts, \n",
    "               extract(h from ts) AS hour, \n",
    "               extract(d from ts) AS day, \n",
    "               extract(w from ts) AS week, \n",
    "               extract(mon from ts) AS month, \n",
    "               extract(year from ts) AS year, \n",
    "               extract(dow from ts) AS weekday\n",
    "               FROM staging_events WHERE ts IS NOT NULL\n",
    "\"\"\")\n",
    "cur.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"INSERT INTO dim_time(start_time, hour, day, week, month, year, weekday)\n",
    "               SELECT DISTINCT ts, \n",
    "               extract(h from ts) AS hour, \n",
    "               extract(d from ts) AS day, \n",
    "               extract(w from ts) AS week, \n",
    "               extract(mon from ts) AS month, \n",
    "               extract(year from ts) AS year, \n",
    "               extract(dow from ts) AS weekday\n",
    "               FROM staging_events WHERE ts IS NOT NULL\n",
    "            \"\"\")\n",
    "\n",
    "# Preview newly created dim_time table \n",
    "query = cur.execute(\"\"\"SELECT * FROM dim_time\"\"\")\n",
    "for i in range(3):\n",
    "        row = cur.fetchone()\n",
    "        if row == None:\n",
    "            break\n",
    "        print(row)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join each dim table to the fact table and preview it\n",
    "\n",
    "cur.execute(\"\"\"SELECT * FROM fact_songplay fs\n",
    "JOIN dim_users on fs.user_id = dim_users.user_id\n",
    "JOIN dim_artists on fs.artist_id = dim_artists.artist_id\n",
    "JOIN dim_songs on fs.song_id = dim_songs.song_id\n",
    "JOIN dim_time on fs.start_time = dim_time.start_time\n",
    "\"\"\")\n",
    "df = pd.DataFrame(cur.fetchone()).T\n",
    "\n",
    "\n",
    "for value in list(df.values[0]):\n",
    "    print(value)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete your cluster and resources after no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Cluster\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check deletion status\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '7b0f3bf9-c99b-4b0e-a6b1-6c3871407371',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7b0f3bf9-c99b-4b0e-a6b1-6c3871407371',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '200',\n",
       "   'date': 'Tue, 03 Aug 2021 17:41:04 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detach role policy & DELETE role\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=\"DWH_IAM_ROLE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
